{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "sys.version_info(major=3, minor=6, micro=12, releaselevel='final', serial=0)\n",
      "matplotlib 3.3.2\n",
      "numpy 1.18.5\n",
      "pandas 1.1.3\n",
      "sklearn 0.21.2\n",
      "tensorflow 2.3.1\n",
      "tensorflow.keras 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state = 7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train_all, y_train_all, random_state = 7)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.fit_transform(x_valid)\n",
    "x_test_scaled = scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'generate_csv'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "def save_2_csv(output_dir, data, name_prefix, header=None, n_parts=10):\n",
    "    path_format = os.path.join(output_dir, '{}__{:02d}.csv')\n",
    "    filenames = []\n",
    "    \n",
    "    for file_idx, row_indices in enumerate(\n",
    "        np.array_split(np.arange(len(data)), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filenames.append(part_csv)\n",
    "        with open(part_csv, 'wt', encoding='utf-8') as f:\n",
    "            if header:\n",
    "                f.write(header+'\\n')\n",
    "            for row_indice in row_indices:\n",
    "                f.write(','.join([repr(col) for col in data[row_indice]]))\n",
    "                f.write('\\n')\n",
    "    \n",
    "    return filenames\n",
    "    \n",
    "\n",
    "train_data = np.c_[x_train_scaled, y_train]\n",
    "valid_data = np.c_[x_valid_scaled, y_valid]\n",
    "test_data = np.c_[x_test_scaled, y_test]\n",
    "header_cols = housing.feature_names + ['MidianHouseValue']\n",
    "header_str = ','.join(header_cols)\n",
    "\n",
    "train_filenames = save_2_csv(output_dir, train_data, 'train', header_str, n_parts=20)\n",
    "valid_filenames = save_2_csv(output_dir, valid_data, 'valid', header_str, n_parts=10)\n",
    "test_filenames = save_2_csv(output_dir, test_data, 'test', header_str, n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train filenames\n",
      "['generate_csv/train__00.csv',\n",
      " 'generate_csv/train__01.csv',\n",
      " 'generate_csv/train__02.csv',\n",
      " 'generate_csv/train__03.csv',\n",
      " 'generate_csv/train__04.csv',\n",
      " 'generate_csv/train__05.csv',\n",
      " 'generate_csv/train__06.csv',\n",
      " 'generate_csv/train__07.csv',\n",
      " 'generate_csv/train__08.csv',\n",
      " 'generate_csv/train__09.csv',\n",
      " 'generate_csv/train__10.csv',\n",
      " 'generate_csv/train__11.csv',\n",
      " 'generate_csv/train__12.csv',\n",
      " 'generate_csv/train__13.csv',\n",
      " 'generate_csv/train__14.csv',\n",
      " 'generate_csv/train__15.csv',\n",
      " 'generate_csv/train__16.csv',\n",
      " 'generate_csv/train__17.csv',\n",
      " 'generate_csv/train__18.csv',\n",
      " 'generate_csv/train__19.csv']\n",
      "valid filenames\n",
      "['generate_csv/valid__00.csv',\n",
      " 'generate_csv/valid__01.csv',\n",
      " 'generate_csv/valid__02.csv',\n",
      " 'generate_csv/valid__03.csv',\n",
      " 'generate_csv/valid__04.csv',\n",
      " 'generate_csv/valid__05.csv',\n",
      " 'generate_csv/valid__06.csv',\n",
      " 'generate_csv/valid__07.csv',\n",
      " 'generate_csv/valid__08.csv',\n",
      " 'generate_csv/valid__09.csv']\n",
      "test filenames\n",
      "['generate_csv/test__00.csv',\n",
      " 'generate_csv/test__01.csv',\n",
      " 'generate_csv/test__02.csv',\n",
      " 'generate_csv/test__03.csv',\n",
      " 'generate_csv/test__04.csv',\n",
      " 'generate_csv/test__05.csv',\n",
      " 'generate_csv/test__06.csv',\n",
      " 'generate_csv/test__07.csv',\n",
      " 'generate_csv/test__08.csv',\n",
      " 'generate_csv/test__09.csv']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "print('train filenames')\n",
    "pprint.pprint(train_filenames)\n",
    "print('valid filenames')\n",
    "pprint.pprint(valid_filenames)\n",
    "print('test filenames')\n",
    "pprint.pprint(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'generate_csv/train__00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train__02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train__19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train__17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train__09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train__15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train__03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train__13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train__16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train__10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train__05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train__07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train__08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train__18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train__06.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train__14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train__01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train__12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train__11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate_csv/train__04.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# 1.filename -> dataset\n",
    "# 2.read file -> dataset -> datasets -> merge\n",
    "\n",
    "filename_dataset = tf.data.Dataset.list_files(train_filenames)\n",
    "for filename in filename_dataset:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'-1.10414274616804,0.026095981445111813,-0.4697282482241662,-0.04522296329387594,1.278182703423561,0.15140218773160447,-0.1958179121763065,0.3402905329813301,0.455'\n",
      "b'1.155819426536407,-0.13344817051789326,0.6085721468637671,0.04220941749296647,1.167006966211348,0.04454635296762321,-0.7388499983026352,0.9087808157853589,2.148'\n",
      "b'0.24184624203240349,1.6215375010751625,0.009664811030936506,-0.05126299611752959,0.0030306872168157756,-0.12848071177796017,-0.7575752426518185,0.5547210782495127,5.00001'\n",
      "b'-1.0163746567322829,1.86085372901967,-0.8985310852575602,-0.053064742407918725,-0.8097313310391347,0.2399721267795869,-0.7669378648264085,0.6544562155835542,0.388'\n",
      "b'-0.5153188550167574,1.6215375010751625,-0.5674383914596058,-0.09192162409389779,-0.6101203483172071,-0.1362930022629114,1.0353669037825188,-1.335259774230543,2.113'\n",
      "b'-1.4627275021629542,-0.691852702388411,-1.5381215558019357,-0.10629815553305554,-1.1045154827381842,-0.17304110653435698,0.8013013494177228,-1.1507497701625737,1.875'\n",
      "b'0.8274958804011991,-0.37276439846240084,-0.021131350700892792,-0.2520319060913993,1.9907181101018345,0.11129897766356796,0.8106639715923161,-1.1756835544960822,2.641'\n",
      "b'-0.9716176924992204,-0.691852702388411,-0.2596016484740199,-0.1397123594639106,-0.5393721519094352,-0.07717107637960378,1.9856730545035917,-0.6670343540924798,0.709'\n",
      "b'-1.0483288014093157,0.4249563613526245,-1.0226767825256236,-0.041374499138598725,1.5957073468251086,0.048038660282047556,-0.7388499983026352,0.6444827018501521,2.475'\n",
      "b'-1.2395775315541393,1.86085372901967,-1.1683362207894175,0.022679365237900546,0.2161175168735572,-0.06696920715665644,0.9979164150841523,-1.4050743703643713,2.375'\n",
      "b'0.06377066093489951,0.4249563613526245,0.18708516335642836,-0.15420561914317354,-0.3001758688164922,-0.10196428768589638,1.3490147466313465,-0.9562662523611952,1.648'\n",
      "b'0.20703526985113252,0.504728437334127,0.4585703355218813,0.053598035056311756,-0.3877688738927812,-0.010882217954068915,0.9089715044255309,-1.4499551821646912,2.417'\n",
      "b'-0.11414620752348001,1.0631329692046447,-0.07279432778010213,0.02413199105865309,-0.010445159717997912,-0.042861839895142875,0.9932351039968573,-1.4449684252979866,3.284'\n",
      "b'-0.8898277791371465,1.3024491971491523,-0.21717301875721895,-0.14899335962078586,-0.4273541742637964,-0.0002746105336503454,0.974509859647674,-1.3951008566309693,1.853'\n",
      "b'-0.9064397020321601,0.5845005133156296,-0.6994545730611924,-0.10441338979964694,-0.10477608826169373,0.18504601792673156,-0.7482126204772251,0.7791251372511042,1.621'\n"
     ]
    }
   ],
   "source": [
    "n_readers = 5\n",
    "dataset = filename_dataset.interleave(\n",
    "    lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
    "    cycle_length = n_readers\n",
    ")\n",
    "for line in dataset.take(15):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
