{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "sys.version_info(major=3, minor=6, micro=12, releaselevel='final', serial=0)\n",
      "matplotlib 3.3.2\n",
      "numpy 1.18.5\n",
      "pandas 1.1.3\n",
      "sklearn 0.21.2\n",
      "tensorflow 2.3.1\n",
      "tensorflow.keras 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 100), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = tf.keras.layers.Dense(100)\n",
    "layer = tf.keras.layers.Dense(100, input_shape=[None, 5])\n",
    "layer(tf.zeros([10, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_1/kernel:0' shape=(5, 100) dtype=float32, numpy=\n",
       " array([[ 1.13292053e-01, -2.72256434e-02, -1.35742486e-01,\n",
       "          7.84245133e-03,  7.85802156e-02, -1.84682101e-01,\n",
       "         -2.33658642e-01,  8.58099312e-02,  4.62632924e-02,\n",
       "         -5.02004623e-02, -9.56479460e-02, -3.93975675e-02,\n",
       "          2.18234763e-01,  8.43286961e-02, -1.69373304e-02,\n",
       "          1.17119744e-01, -2.27364287e-01, -1.65052116e-02,\n",
       "         -9.05334949e-04, -2.16028437e-01, -1.07879773e-01,\n",
       "          1.85022280e-01,  1.88538238e-01,  8.77775103e-02,\n",
       "         -1.85592920e-02,  1.14331588e-01, -2.37621069e-01,\n",
       "         -1.55450702e-01, -2.15154916e-01,  1.10046417e-02,\n",
       "         -1.42247140e-01,  2.37682328e-01, -2.05071598e-01,\n",
       "          1.96422085e-01,  2.24133238e-01,  8.80009085e-02,\n",
       "         -1.83500290e-01, -1.41707212e-02, -1.53819799e-01,\n",
       "          2.27520511e-01, -1.20986156e-01,  7.77297765e-02,\n",
       "         -2.08389968e-01,  9.85053033e-02,  2.01942906e-01,\n",
       "          1.74233243e-01, -1.35886163e-01, -1.02420822e-01,\n",
       "         -1.79740340e-02, -2.29441509e-01, -1.10168159e-01,\n",
       "         -1.77946553e-01,  2.23501995e-01, -2.16537267e-01,\n",
       "         -4.82983887e-02,  7.59197772e-03, -5.89984655e-03,\n",
       "         -2.21851915e-02, -4.75014001e-02,  2.35732645e-02,\n",
       "          1.94644645e-01, -2.08432198e-01, -9.52320099e-02,\n",
       "          1.38473764e-01, -7.52195716e-02, -5.23577631e-02,\n",
       "         -2.26259589e-01, -1.95635974e-01,  1.00788191e-01,\n",
       "         -7.15696216e-02,  2.08368078e-01,  1.29998550e-01,\n",
       "         -3.64742279e-02,  1.56801596e-01, -9.69393551e-02,\n",
       "         -8.58165324e-03,  1.35746256e-01,  8.34293514e-02,\n",
       "          7.79732913e-02,  4.71790880e-02,  1.41521618e-01,\n",
       "         -5.51678538e-02, -1.51153952e-01,  1.11357883e-01,\n",
       "          1.13060430e-01, -1.67653799e-01,  2.30818048e-01,\n",
       "          9.46264714e-02,  4.84034419e-04,  1.57488033e-01,\n",
       "         -5.53763956e-02,  2.27845803e-01,  1.78936586e-01,\n",
       "         -1.27873465e-01, -2.11817980e-01, -7.19514787e-02,\n",
       "         -2.08256081e-01, -1.83174059e-01, -1.99332640e-01,\n",
       "         -1.52469113e-01],\n",
       "        [-1.35545522e-01,  1.95680544e-01,  1.20674178e-01,\n",
       "          8.01644474e-02, -5.69057465e-03,  1.34643331e-01,\n",
       "         -1.36705607e-01, -3.91936451e-02,  7.31124133e-02,\n",
       "         -4.38222736e-02,  1.71232894e-01,  1.70160189e-01,\n",
       "          1.83129892e-01, -1.60371244e-01,  1.35971263e-01,\n",
       "         -2.36372471e-01,  2.32994780e-01, -1.17946863e-02,\n",
       "         -9.39192325e-02, -1.82943642e-01,  8.02627653e-02,\n",
       "          8.81500691e-02,  6.60738498e-02,  2.16643408e-01,\n",
       "         -2.28976667e-01,  1.70282707e-01,  1.37973025e-01,\n",
       "         -1.06953010e-01, -1.38389528e-01, -4.33769822e-02,\n",
       "          1.23873487e-01,  1.45238206e-01,  1.62377611e-01,\n",
       "          1.97674558e-01, -2.28922635e-01, -1.51817799e-02,\n",
       "          3.42634767e-02,  3.49065214e-02,  9.15875584e-02,\n",
       "         -2.05790460e-01,  1.89377815e-02,  6.91448599e-02,\n",
       "          2.04319760e-01,  3.56416255e-02,  1.80430129e-01,\n",
       "          8.77362937e-02,  1.06445700e-02, -1.01577774e-01,\n",
       "         -1.61318630e-01, -2.16928020e-01,  1.40995339e-01,\n",
       "          2.38283321e-01, -1.84705123e-01,  4.26882356e-02,\n",
       "         -4.07578796e-02, -1.57550544e-01, -2.20494241e-01,\n",
       "         -1.10195562e-01, -1.90658420e-02, -2.35976085e-01,\n",
       "          2.32167199e-01, -1.76938295e-01, -2.37601697e-01,\n",
       "          4.43546325e-02,  2.66812891e-02,  1.86048284e-01,\n",
       "         -1.30267173e-01, -1.85725749e-01,  1.74524412e-01,\n",
       "         -2.34268636e-01,  1.81647107e-01, -7.63549209e-02,\n",
       "          1.63286492e-01, -2.03196019e-01, -8.57974589e-02,\n",
       "         -2.00521797e-01, -7.60667026e-03,  2.55292505e-02,\n",
       "         -1.12894982e-01,  6.41560405e-02, -3.27174366e-03,\n",
       "          3.23970169e-02, -8.46202075e-02, -1.91453993e-01,\n",
       "         -2.01188475e-02, -2.70685107e-02,  2.08532661e-02,\n",
       "          1.48064807e-01, -2.28991255e-01,  1.62624672e-01,\n",
       "         -1.10687524e-01, -1.82105273e-01, -1.90045863e-01,\n",
       "         -2.04400450e-01,  1.44938692e-01,  1.67151526e-01,\n",
       "          8.12952369e-02, -1.33211434e-01,  9.15882736e-02,\n",
       "          5.63956797e-03],\n",
       "        [ 7.93326944e-02, -2.54496783e-02, -4.76584136e-02,\n",
       "          1.74251005e-01, -8.57905000e-02, -9.47012901e-02,\n",
       "          1.64237574e-01, -8.96781087e-02,  1.75838277e-01,\n",
       "         -1.00085184e-01,  1.12292022e-02,  1.87881738e-02,\n",
       "          3.31358165e-02, -1.13359079e-01, -1.92146003e-01,\n",
       "          1.46518365e-01,  2.43991464e-02,  2.21807763e-01,\n",
       "         -1.28213763e-01,  8.57730061e-02,  1.06729940e-01,\n",
       "          7.74440020e-02,  1.33501440e-02,  1.16822019e-01,\n",
       "          2.03255817e-01,  2.03643009e-01,  1.50287703e-01,\n",
       "         -1.31413072e-01,  1.82655886e-01,  2.30986342e-01,\n",
       "          1.26945630e-01, -1.49208784e-01, -6.33551180e-03,\n",
       "         -9.50402915e-02, -1.21362135e-01, -1.48358375e-01,\n",
       "         -5.59223890e-02, -1.66168898e-01, -4.74163890e-03,\n",
       "          2.08206490e-01,  1.05036512e-01,  2.13292256e-01,\n",
       "          1.73575774e-01, -8.64886642e-02,  2.17910260e-02,\n",
       "         -7.37208873e-02, -4.23728824e-02,  4.24314588e-02,\n",
       "         -2.28246450e-02,  1.83166429e-01, -1.29860416e-01,\n",
       "          1.37044385e-01, -2.36149460e-01, -4.94344831e-02,\n",
       "         -1.35352254e-01,  2.18077615e-01,  2.10322291e-02,\n",
       "         -5.14025092e-02,  6.39660805e-02,  2.93769091e-02,\n",
       "          7.84391612e-02,  1.02665767e-01, -1.76735163e-01,\n",
       "         -3.36256027e-02, -1.16067722e-01, -2.07633376e-01,\n",
       "          2.27159962e-01, -3.39817554e-02,  1.96875781e-02,\n",
       "         -2.66249329e-02,  1.92677751e-01, -1.75636292e-01,\n",
       "         -1.01982594e-01, -4.96005565e-02,  1.91004083e-01,\n",
       "         -1.32269740e-01,  1.90391019e-01, -9.39709246e-02,\n",
       "          2.18826681e-02, -1.09564036e-01, -1.44887909e-01,\n",
       "         -5.00960052e-02,  2.01827213e-01, -1.26659185e-01,\n",
       "          2.01254115e-01,  3.66211087e-02, -1.68714672e-01,\n",
       "         -7.27096051e-02, -7.40486532e-02,  2.18942538e-01,\n",
       "          2.15293691e-01, -2.07087278e-01,  1.85744330e-01,\n",
       "          1.21880159e-01,  3.54088098e-02,  1.44414648e-01,\n",
       "          7.60485381e-02, -6.13008142e-02, -1.00820690e-01,\n",
       "          1.34858176e-01],\n",
       "        [-1.02833331e-01,  1.95881769e-01, -2.07710654e-01,\n",
       "          9.11522955e-02,  1.77067623e-01, -5.26361167e-02,\n",
       "          3.55431437e-03, -1.62524894e-01, -9.65299159e-02,\n",
       "          6.18119091e-02, -1.81641355e-01, -1.31559670e-02,\n",
       "         -1.38303638e-02,  1.56719700e-01, -1.10982358e-01,\n",
       "          9.55971032e-02,  7.35918432e-02,  4.64989394e-02,\n",
       "         -3.20765972e-02, -1.20327659e-01,  1.59678653e-01,\n",
       "          2.01354071e-01, -1.38953239e-01, -1.84726551e-01,\n",
       "          6.31809980e-02,  1.91219643e-01, -2.08117366e-01,\n",
       "          1.57492474e-01,  2.21415296e-01, -2.18415648e-01,\n",
       "          1.61641374e-01, -5.55493683e-02, -1.91054419e-01,\n",
       "         -5.11580706e-03,  1.83512315e-01,  9.39058512e-02,\n",
       "          5.73235005e-02,  8.00441951e-02,  6.10057563e-02,\n",
       "         -3.64618003e-03, -1.93356991e-01,  1.55736282e-01,\n",
       "         -1.24503188e-01,  1.38287097e-02,  3.53670120e-03,\n",
       "          5.39181083e-02,  1.46382898e-02, -3.77255678e-02,\n",
       "          2.03276947e-01,  1.48973912e-02,  1.44535229e-01,\n",
       "         -1.05526075e-01, -1.26903281e-01, -1.97508186e-01,\n",
       "         -1.68694660e-01, -2.24899113e-01,  1.39985368e-01,\n",
       "          6.21426553e-02, -1.64109856e-01,  1.20254591e-01,\n",
       "          7.74371177e-02, -1.85614049e-01, -1.55411035e-01,\n",
       "         -1.17882550e-01, -9.47773755e-02, -4.99585867e-02,\n",
       "         -7.98040777e-02,  1.77108005e-01,  7.43526369e-02,\n",
       "         -1.35573447e-01, -7.68623352e-02,  1.28031567e-01,\n",
       "          1.23176917e-01,  1.40853330e-01,  1.75112054e-01,\n",
       "          2.22464725e-01,  9.69392508e-02,  6.59355968e-02,\n",
       "         -2.24521190e-01, -7.92156905e-02,  1.10455617e-01,\n",
       "          1.99749038e-01, -2.37068817e-01, -1.94556519e-01,\n",
       "         -1.21749349e-01,  1.98292479e-01, -1.94040686e-01,\n",
       "          8.52333456e-02, -6.87015057e-03, -1.49643123e-01,\n",
       "          1.41605243e-01,  7.09556043e-03, -6.25703782e-02,\n",
       "          2.11529598e-01,  1.59152225e-01, -2.08344609e-02,\n",
       "         -9.59185511e-02,  1.85087815e-01, -1.18214764e-01,\n",
       "          8.81085843e-02],\n",
       "        [ 3.80060226e-02,  1.22571126e-01,  1.66561976e-01,\n",
       "          1.18079677e-01, -2.02803120e-01,  2.28383943e-01,\n",
       "         -1.79171965e-01, -2.73901820e-02,  4.30980325e-03,\n",
       "          1.27787456e-01,  1.40246853e-01, -1.19664669e-02,\n",
       "          1.22795269e-01,  1.75232336e-01, -1.62508070e-01,\n",
       "         -2.17465162e-02,  1.85263649e-01,  2.44448334e-02,\n",
       "         -1.81802467e-01,  1.42145351e-01,  1.62103370e-01,\n",
       "         -5.03611863e-02,  1.50654927e-01,  1.48679927e-01,\n",
       "          1.81695893e-01,  1.44644424e-01, -1.77343175e-01,\n",
       "          1.63036302e-01, -1.19474821e-01,  2.20307693e-01,\n",
       "         -1.18364364e-01,  1.68290243e-01, -2.22713709e-01,\n",
       "          5.44600040e-02,  1.28541604e-01, -1.56609982e-01,\n",
       "         -1.37632877e-01, -1.99722409e-01, -6.26385510e-02,\n",
       "          2.19447270e-01,  1.55113861e-01, -1.52416512e-01,\n",
       "         -2.04343975e-01,  2.16111824e-01,  1.91753551e-01,\n",
       "         -2.01941669e-01,  2.25635335e-01,  1.24772593e-01,\n",
       "         -3.11045349e-03, -2.32189894e-04,  6.88074082e-02,\n",
       "         -2.17839509e-01, -1.79749250e-01, -3.24047059e-02,\n",
       "          4.68855947e-02,  7.76705146e-03, -2.18363672e-01,\n",
       "          2.30627209e-02,  1.72950283e-01, -9.44338292e-02,\n",
       "         -1.00414380e-01,  1.64535388e-01,  2.07453743e-01,\n",
       "          1.88103721e-01, -1.17592908e-01, -8.00892115e-02,\n",
       "          1.28431305e-01, -8.91728103e-02,  2.10890755e-01,\n",
       "          1.56804636e-01, -4.77407575e-02,  4.32203263e-02,\n",
       "          1.43611431e-03, -1.83415532e-01, -2.14952871e-01,\n",
       "          9.21991915e-02,  9.23160762e-02, -5.66879213e-02,\n",
       "         -2.19288915e-02, -8.50114077e-02, -1.46425068e-02,\n",
       "          2.17648461e-01,  1.15690187e-01,  1.26578465e-01,\n",
       "         -1.51902556e-01, -2.92084217e-02, -2.14181185e-01,\n",
       "          1.18872508e-01,  9.16068703e-02, -1.91941679e-01,\n",
       "         -1.49637312e-01, -2.30711594e-01, -2.15205550e-03,\n",
       "         -1.95533320e-01, -2.96060741e-03,  1.22219428e-01,\n",
       "          4.18524891e-02, -4.20560539e-02,  2.01608613e-01,\n",
       "         -2.18477204e-01]], dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(100,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer.variables\n",
    "# x  * w + b\n",
    "layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Dense in module tensorflow.python.keras.layers.core object:\n",
      "\n",
      "class Dense(tensorflow.python.keras.engine.base_layer.Layer)\n",
      " |  Just your regular densely-connected NN layer.\n",
      " |  \n",
      " |  `Dense` implements the operation:\n",
      " |  `output = activation(dot(input, kernel) + bias)`\n",
      " |  where `activation` is the element-wise activation function\n",
      " |  passed as the `activation` argument, `kernel` is a weights matrix\n",
      " |  created by the layer, and `bias` is a bias vector created by the layer\n",
      " |  (only applicable if `use_bias` is `True`).\n",
      " |  \n",
      " |  Note: If the input to the layer has a rank greater than 2, then `Dense`\n",
      " |  computes the dot product between the `inputs` and the `kernel` along the\n",
      " |  last axis of the `inputs` and axis 1 of the `kernel` (using `tf.tensordot`).\n",
      " |  For example, if input has dimensions `(batch_size, d0, d1)`,\n",
      " |  then we create a `kernel` with shape `(d1, units)`, and the `kernel` operates\n",
      " |  along axis 2 of the `input`, on every sub-tensor of shape `(1, 1, d1)`\n",
      " |  (there are `batch_size * d0` such sub-tensors).\n",
      " |  The output in this case will have shape `(batch_size, d0, units)`.\n",
      " |  \n",
      " |  Besides, layer attributes cannot be modified after the layer has been called\n",
      " |  once (except the `trainable` attribute).\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  >>> # Create a `Sequential` model and add a Dense layer as the first layer.\n",
      " |  >>> model = tf.keras.models.Sequential()\n",
      " |  >>> model.add(tf.keras.Input(shape=(16,)))\n",
      " |  >>> model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
      " |  >>> # Now the model will take as input arrays of shape (None, 16)\n",
      " |  >>> # and output arrays of shape (None, 32).\n",
      " |  >>> # Note that after the first layer, you don't need to specify\n",
      " |  >>> # the size of the input anymore:\n",
      " |  >>> model.add(tf.keras.layers.Dense(32))\n",
      " |  >>> model.output_shape\n",
      " |  (None, 32)\n",
      " |  \n",
      " |  Arguments:\n",
      " |    units: Positive integer, dimensionality of the output space.\n",
      " |    activation: Activation function to use.\n",
      " |      If you don't specify anything, no activation is applied\n",
      " |      (ie. \"linear\" activation: `a(x) = x`).\n",
      " |    use_bias: Boolean, whether the layer uses a bias vector.\n",
      " |    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
      " |    bias_initializer: Initializer for the bias vector.\n",
      " |    kernel_regularizer: Regularizer function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_regularizer: Regularizer function applied to the bias vector.\n",
      " |    activity_regularizer: Regularizer function applied to\n",
      " |      the output of the layer (its \"activation\").\n",
      " |    kernel_constraint: Constraint function applied to\n",
      " |      the `kernel` weights matrix.\n",
      " |    bias_constraint: Constraint function applied to the bias vector.\n",
      " |  \n",
      " |  Input shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
      " |    The most common situation would be\n",
      " |    a 2D input with shape `(batch_size, input_dim)`.\n",
      " |  \n",
      " |  Output shape:\n",
      " |    N-D tensor with shape: `(batch_size, ..., units)`.\n",
      " |    For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
      " |    the output would have shape `(batch_size, units)`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dense\n",
      " |      tensorflow.python.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      tensorflow.python.keras.utils.version_utils.LayerVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Note here that `call()` method in `tf.keras` is little bit different\n",
      " |      from `keras` API. In `keras` API, you can pass support masking for\n",
      " |      layers as additional arguments. Whereas `tf.keras` has `compute_mask()`\n",
      " |      method to support masking.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments. Currently unused.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(self, inputs):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      d = tf.keras.layers.Dense(10)\n",
      " |      x = d(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |            inputs - Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_metric(self, value, name=None, **kwargs)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      This method can be used inside the `call()` method of a subclassed layer\n",
      " |      or model.\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
      " |        def __init__(self):\n",
      " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      " |          self.mean = metrics_module.Mean(name='metric_1')\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          self.add_metric(self.mean(x))\n",
      " |          self.add_metric(math_ops.reduce_sum(x), name='metric_2')\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any tensor passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      metrics become part of the model's topology and are tracked when you\n",
      " |      save the model via `save()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
      " |      Functional Model, as shown in the example below, is not supported. This is\n",
      " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        name: String metric name.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |          `aggregation` - When the `value` tensor provided is not the result of\n",
      " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
      " |          using a `keras.Metric.Mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      `inputs` is now automatically inferred\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.add_weight` method instead.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        partitioner: Partitioner to be passed to the `Trackable` API.\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable. Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partitioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.__call__` method instead.\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.losses` instead.\n",
      " |      \n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.updates` instead.\n",
      " |      \n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with this\n",
      " |      layer as a list of Numpy arrays, which can in turn be used to load state\n",
      " |      into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a Dense layer returns a list of two values-- per-output\n",
      " |      weights and the bias value. These can be used to set the weights of another\n",
      " |      Dense layer:\n",
      " |      \n",
      " |      >>> a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b.set_weights(a.get_weights())\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a Dense layer returns a list of two values-- per-output\n",
      " |      weights and the bias value. These can be used to set the weights of another\n",
      " |      Dense layer:\n",
      " |      \n",
      " |      >>> a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b.set_weights(a.get_weights())\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  dtype\n",
      " |      Dtype used by the weights of the layer, set in the constructor.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
      " |      the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
      " |      of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
      " |      error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  losses\n",
      " |      List of losses added using the `add_loss()` API.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
      " |      ...   def call(self, inputs):\n",
      " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |      ...     return inputs\n",
      " |      >>> l = MyLayer()\n",
      " |      >>> l(np.ones((10, 1)))\n",
      " |      >>> l.losses\n",
      " |      [1.0]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Activity regularization.\n",
      " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor 'Abs:0' shape=() dtype=float32>]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      " |      >>> x = d(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Weight regularization.\n",
      " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of metrics added using the `add_metric()` API.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> input = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2)\n",
      " |      >>> output = d(input)\n",
      " |      >>> d.add_metric(tf.reduce_max(output), name='max')\n",
      " |      >>> d.add_metric(tf.reduce_min(output), name='min')\n",
      " |      >>> [m.name for m in d.metrics]\n",
      " |      ['max', 'min']\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are expected\n",
      " |      to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |      DEPRECATED FUNCTION\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from tensorflow.python.keras.utils.version_utils.LayerVersionSelector:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _california_housing_dataset:\n",
      "\n",
      "California Housing dataset\n",
      "--------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 20640\n",
      "\n",
      "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
      "\n",
      "    :Attribute Information:\n",
      "        - MedInc        median income in block\n",
      "        - HouseAge      median house age in block\n",
      "        - AveRooms      average number of rooms\n",
      "        - AveBedrms     average number of bedrooms\n",
      "        - Population    block population\n",
      "        - AveOccup      average house occupancy\n",
      "        - Latitude      house block latitude\n",
      "        - Longitude     house block longitude\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "This dataset was obtained from the StatLib repository.\n",
      "http://lib.stat.cmu.edu/datasets/\n",
      "\n",
      "The target variable is the median house value for California districts.\n",
      "\n",
      "This dataset was derived from the 1990 U.S. census, using one row per census\n",
      "block group. A block group is the smallest geographical unit for which the U.S.\n",
      "Census Bureau publishes sample data (a block group typically has a population\n",
      "of 600 to 3,000 people).\n",
      "\n",
      "It can be downloaded/loaded using the\n",
      ":func:`sklearn.datasets.fetch_california_housing` function.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
      "      Statistics and Probability Letters, 33 (1997) 291-297\n",
      "\n",
      "(20640, 8)\n",
      "(20640,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "print(housing.DESCR)\n",
    "print(housing.data.shape)\n",
    "print(housing.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (11610,)\n",
      "(3870, 8) (3870,)\n",
      "(5160, 8) (5160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(\n",
    "    housing.data, housing.target, random_state = 7)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    x_train_all, y_train_all, random_state = 11)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_valid.shape, y_valid.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_valid_scaled = scaler.transform(x_valid)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4.5398901e-05 6.7153485e-03 6.9314718e-01 5.0067153e+00 1.0000046e+01], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.nn.softplus :log(1+e^x)\n",
    "customized_softplus = keras.layers.Lambda(lambda x : tf.nn.softplus(x))\n",
    "print(customized_softplus([-10., -5., 0., 5., 10.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "customized_dense_layer_4 (Cu (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "customized_dense_layer_5 (Cu (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# customized dense layer\n",
    "class CustomizedDenseLayer(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        self.units = units\n",
    "        self.activation = keras.layers.Activation(activation)\n",
    "        super(CustomizedDenseLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\"\"\"\n",
    "        # x * w + b. input_Sjape:[None, a] w:[1, b] output_shape:[None, b]      \n",
    "        self.kernel = self.add_weight(name = 'kernel',\n",
    "                                     shape = (input_shape[1], self.units),\n",
    "                                     initializer = 'uniform',\n",
    "                                     trainable = True)\n",
    "        self.bias = self.add_weight(name = 'bias',\n",
    "                                   shape = (self.units, ),\n",
    "                                   initializer= 'zeros',\n",
    "                                   trainable=True)\n",
    "        \n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\"\"\"\"\n",
    "        return self.activation(x @ self.kernel + self.bias)\n",
    "    \n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    CustomizedDenseLayer(30, activation='relu',input_shape=x_train.shape[1:]),\n",
    "    CustomizedDenseLayer(1),\n",
    "    customized_softplus,\n",
    "    # keras.layers.Dense(1, activation='softplus')     \n",
    "    # keras.layers.Dense(1), keras.layers.Dense('spftplus'),    \n",
    "]) \n",
    "model.summary()\n",
    "model.compile(loss='mean_squared_error', optimizer = 'sgd')\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=5, min_delta=1e-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.0522 - val_loss: 0.6248\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5417 - val_loss: 0.5356\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4821 - val_loss: 0.5108\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4672 - val_loss: 0.4744\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4452 - val_loss: 0.4877\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4501 - val_loss: 0.4533\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4307 - val_loss: 0.4408\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4175 - val_loss: 0.4330\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4222 - val_loss: 0.4334\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4066 - val_loss: 0.4167\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3979 - val_loss: 0.4093\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3945 - val_loss: 0.4062\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3888 - val_loss: 0.4023\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3846 - val_loss: 0.4005\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3824 - val_loss: 0.3943\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3815 - val_loss: 0.3931\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3854 - val_loss: 0.3974\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3791 - val_loss: 0.4336\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3788 - val_loss: 0.3881\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3754 - val_loss: 0.3924\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train_scaled, y_train, validation_data = (x_valid_scaled, y_valid), epochs=100, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learning_curvres(history):\n",
    "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curvres(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 756us/step - loss: 0.3946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3946301341056824"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
